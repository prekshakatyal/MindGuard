# -*- coding: utf-8 -*-
"""Accenture.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NDhAO2XuPjHdjeEPTKwx4-lF0vqSi9K_
"""

import pandas as pd

# Load data
df = pd.read_csv('/content/data.csv')

# Select emotion columns (starts with Answer.f1)
emotion_cols = [col for col in df.columns if 'Answer.f1' in col]

# Create a new column that holds the dominant emotion (the first True value in row)
def get_emotion(row):
    for col in emotion_cols:
        if row[col]:
            return col.split('.')[-2]  # Get the emotion name
    return 'neutral'  # fallback if no emotion is marked True

# Apply the function
df['emotion'] = df.apply(get_emotion, axis=1)

# Show results
print(df[['Answer', 'emotion']].head())
print("\nüîç Unique Emotions:", df['emotion'].unique())
print("\nüìä Distribution:\n", df['emotion'].value_counts())

!pip install transformers datasets --quiet

import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import RobertaTokenizer

# Load and split
df = pd.read_csv('/content/data.csv')
emotion_cols = [col for col in df.columns if 'Answer.f1' in col]

def get_emotion(row):
    for col in emotion_cols:
        if row[col]:
            return col.split('.')[-2]
    return 'neutral'

df['emotion'] = df.apply(get_emotion, axis=1)
df = df[['Answer', 'emotion']]

# Encode labels
label2id = {label: idx for idx, label in enumerate(df['emotion'].unique())}
# Drop emotions with < 2 samples
df = df.groupby('emotion').filter(lambda x: len(x) > 1).copy()

id2label = {idx: label for label, idx in label2id.items()}
df['label'] = df['emotion'].map(label2id)

# Split
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)

# Hugging Face dataset
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# Tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

def tokenize_function(example):
    return tokenizer(example["Answer"], truncation=True, padding="max_length", max_length=128)

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

# Load the model with correct number of labels
model = AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=18)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)

# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer
)

print("Expected num_labels:", model.config.num_labels)

trainer.train()

